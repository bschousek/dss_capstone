---
title: "JHU DSS Capstone Milestone Report"
author: "Brian Schousek"
date: "July, 2015"
output: html_document
---
```{r libload,echo =F}
library(knitr)
library(plyr)
prettyK=function(value) {formatC(value,big.mark=",",format='d')}

```
## Introduction

This report describes exploratory analysis performed in support of the first milestone in the Coursera Data Science Specialization capstone project. The project's end output is a predictive text application implemented in Shiny. 

## Data Sources
We are charged with exploring data provided by Hans Christensen at [www.corpora.heliohost.org](www.corpora.heliohost.org). The data is collected from a variety of public sources and attempts are made to identify the language of the sources. Although the original source data is tagged for author, source, date among other things, the data we are to use for this exercise has been stripped of the tags. 

The data provided consists of files from four different languages: German, American English, Finnish, and Russian. For each language we are provided with files containing blog entries, Tweets, and news articles. Each file consists of many multiple entries of a given type, with each entry consisting of a paragraph, each of which may contain multiple sentences.

One could envision increasing the reach of a predictive text program by including in final prediction other sources of text data. For example, [Project Gutenberg](www.gutenberg.org) has for many years provided ebooks from a variety of public domain books. The Python module [NLTK](nltk.org) also provides a multidude of corpora which may be useful. For example [Brown Corpus](http://www.hit.uib.no/icame/brown/bcm.html) is included, which contains text samples ranging from sports articles to Science Fiction novels, all collected during the year 1961. The potential market for the eventual predictive text application in this capstone could easily be targeted by choosing training data from these sources and others to match the target market. 

##Natural Language Processing

As in many of the classes in the Data Science Specialization, natural language processing (NLP) attempts extract features from large amounts of data and summarize them using various techniques to facilitate further analysis. In this specific instance we are charged with using this process to predict words, but other uses can include sentiment analysis, source language detection, detection of academic cheating, and many others.

Common low level NLP tasks include:

1. Sentence Boundary Detection
2. Tokenization: Deconstructing text into words, phrases, letters etc. 
3. Part of Speech assignment
4. Morphological decomposition: reducing words to their roots, or in the case of compound words to their component parts
5. Chunking: identifying phrases 

(summarized from Natural language processing: an introduction by Prakash M Nadkarni, at [http://dx.doi.org/10.1136/amiajnl-2011-000464](http://dx.doi.org/10.1136/amiajnl-2011-000464)))

Other tasks can include spelling corrections, language detection and profanity filtering.

 

## Exploratory Analysis


```{r word_counts, echo=F}
source('get_wordcounts.R')
```

### File Summaries
We are provided with files containing news, Tweets, and blog posts in four different languages.  We first report in the table below some basic measures of the blog files from the four languages. It is interesting to note that the Russian blog entries in the data set have on average far fewer words than the English entries. Those words however tend to be longer.

The final column which shows the average number of bytes per text character illustrates another concern with text processing. In plain English text, each letter can ideally be described with one byte of ASCII code. In languages such as German and Finnish the bulk of the letters in the alphabetic can be described with one byte, and only the accented characters require multiple bytes. In the Russian cyrillic, most every character must be represented by two bytes. Note that since the English text is not precisely equal to 1.0 there are some multibyte characters in the text. These could either be non-ASCII punctuation, or some foreign language entries accidentally contained within. In order to keep the application as general purpose as possible special attention must be paid to how we deal with non-ASCII characters. 


```{r wc_summary, echo=F, warning=FALSE, message=FALSE}

maketable=function(wcdf) {
  wcsum=data.frame(language=revalue(wcdf$language,
                                  c("de_DE"="German",
                                    "en_US"="American English",
                                    "fi_FI"='Finnish',
                                    'ru_RU'='Russian')),
                   doctype=wcdf$doctype,
                 lines=prettyK(wcdf$lines),
                 words=prettyK(wcdf$words),
                 avwords=round(wcdf$words/wcdf$lines,1),
                 wordlength=round(wcdf$characters/wcdf$words,1),
                 density=round(wcdf$density,2)
                 )
  wcsum
}
col.names=c("Language","Document Type", "Paragraph count","Total Number of Words",
             "Average words per entry", "average word length",
             "bytes per character")
blog_table=kable(maketable(wcdf_blogs),row.names=F,col.names=col.names)
english_table=kable(maketable(wcdf_english),row.names=F,col.names=col.names)


```
`r blog_table`

### English sources

For the rest of the analysis in this report, we turn our attention to the English files only. The following table shows summary measures for all three varieties of file in English. The major difference here is the relative shortness of each entry in the Twitter file, driven by the 140 character Tweet limit. The bytes per character are similarly low for all three file types, so there is likely not a major difference in inclusion of non-English entries and extra punctuation.

`r english_table`

### Data Analysis

#### Sampling

The first task performed was to combine the texts, then randomly split the entries into bulk training (75%) and test (25%) sets. This leaves more than 2 million entries for training purposes. Part of the exploration in this document and in future work will include estimating how many of these entries will actually need to be part of the final training set. Informed by this estimate, the test dataset will be partitioned into repeatable test and validation sets. For the rest of the analysis in this document, we will take a repeatable, small 50,000 entry sample of the bulk training dataset for further exploratory use. As can be seen in the table below, the sampling maintains with close agreement the simple measures for each of the data sets.

``` {r splitdata, echo=F}
source('split_data.R')
ttsum=data.frame(ttdf$filename,
                 lines=prettyK(ttdf$lines),
                 words=round(ttdf$words/ttdf$lines,1),
                 wordlength=round(ttdf$characters/ttdf$words,1),
                 density=round(ttdf$density,2)
                 )
tttable=kable(ttsum,row.names=F,
              col.names=c("file name","Paragraph count","Average words per entry",
                          "average word length","bytes per character"),
              align='c')                 

```

`r tttable`

#### Tokenization

```{r tokenize, echo=F}
source('tokenize.R')
```

With the exploratory dataset thus trimmed down to reasonable size, we can go about processing the data. A series of Unix command line utilities are applied to clean the text of spurious punctuation, numbers and extra whitespace. In addition the beginning and ends of lines are marked with 'begintoken' and 'endtoken' respectively, to facilitate exploration of *Implementation of Modified Kneser-Ney Smoothing on Top of Generalized Language Models for Next Word Prediction* [http://mkoerner.de/media/bachelor-thesis.pdf](http://mkoerner.de/media/bachelor-thesis.pdf) as pointed to by Trevor Kelley in a forum post for this course. Numbers are replaced with 'numbertoken.'

Profane words are replaced with the token 'kittens' at this time, using a list obtained from [https://gist.github.com/ryanlewis/a37739d710ccdb4b406d](https://gist.github.com/ryanlewis/a37739d710ccdb4b406d). These words were forked by Ryan Lewis from a list scraped from some Google code by Jamie Wilkinson. Frequency lists of single, double, and triple words are also created. In the final trial dataset the word 'kittens' appears `r wordlist$V1[which(wordlist$V2=='kittens')]` times in 50,000 lines. It is uncertain at this time if this is due to the fact that the internet likes to talk about kittens, or because the internet likes to use profanity. Both possibilities may be true.

(The tokenization steps used here were inspired by a website written by Greg Ichneumon Brown at [http://gibrown.com/2013/01/26/unix-bi-grams-tri-grams-and-topic-modeling/](http://gibrown.com/2013/01/26/unix-bi-grams-tri-grams-and-topic-modeling/).)


#### Frequency Analysis

After this processing the data we have to consider includes `r prettyK(length(wordlist$V1))` unique words, `r prettyK(length(bigrams$V1))` bigrams, `r prettyK(length(trigrams$V1))` trigrams, and `r prettyK(length(fourgrams$V1))` fourgrams. Let us look at the distributions of these sets.

The plot below is a rank order plot of word frequencies on a log-log scale. It is very similar to the plot of wikipedia word frequency seen at [https://en.wikipedia.org/wiki/Zipf%27s_law#/media/File:Wikipedia-n-zipf.png](https://en.wikipedia.org/wiki/Zipf%27s_law#/media/File:Wikipedia-n-zipf.png) as part of a wikipedia article about Zipf's law. Zipf's law in it's basic form states that the frequency of a word occurence is inversely proportional to it's location in a ranked list. As the word prediction algorithm proceeds, this relationship may serve to inform tradeoffs on algorithm performance versus completeness of vocabulary. Note, for example, that  the `r prettyK(sum(wordlist$V1==1))` entries which occur only once in the list represent approximately 1/2 of the total of `r prettyK(length(wordlist$V1))` total words. These entries are all compressed on the right lower side of the plot due to the log-log axes.

```{r plothisto, echo=FALSE}
plot(wordlist$V1,log='xy',xlab = 'words', ylab='counts of words', type='p',main = 'Frequency of word occurrence')
```

Similar relationships occur in bi-gram, tri-gram and four-gram frequencies. A bi-gram example is shown below.

```{r plotbigrams, echo=FALSE}
plot(bigrams$V1,log='xy',xlab = 'bi-grams', ylab='counts of bi-grams', type='p',main = 'Frequency of bi-gram occurrence')
```

### Weaknesses in the method

The preparation of the data thus far covers much ground, yet there remain things which could be done. For example, nothing has yet been done to correct common misspellings. Also, in exploring the data it was apparent that there were entries which were filenames, box scores and stock quotes. The natural progression of algorithm development and accuracy testing via rigorous statistical methods will help determine if these shortcomings need to be addressed. It may be possible that low occurrence rates of such strings will not impact final results due to their rarity.


### Future work

With a framework in place for summarizing the data, development of the algorithm can continue. Mentioned earlier, the Martin Körner's thesis *Implementation of Modified Kneser-Ney Smoothing on Top of Generalized Language Models for Next Word Prediction* appears to provide a good low level procedural description of how to proceed with a predictive text algorithm. Some of his suggestions were incorporated into the processing reported within. Although the final algorithm implemented will likely not be the same as that suggested in his thesis, the concepts and references of prediction based on sparse n-gram smoothing will guide in developing an algorithm.

### Notes

This report is intended as an executive summary and as such, details of processing are not rendered within. The complete source code for this document including R code and Linux shell scripts can be found at [http://github.com/bschousek/dss_capstone](http://github.com/bschousek/dss_capstone).

```{r session}
 sessionInfo()
```